```markdown
# Build â†’ Break â†’ Improve: Navigating Synthetic Reality

> Synthetic Image Detector Â· Adversarial Evasion Â· Robustness Defense  
> Dataset: CIFAKE | Model: ResNet-18 Transfer Learning | Framework: PyTorch

---

## Table of Contents

1. [Problem Overview](#1-problem-overview)
2. [Dataset](#2-dataset)
3. [Project Structure](#3-project-structure)
4. [Requirements](#4-requirements)
5. [Setup & Installation](#5-setup--installation)
6. [How to Run](#6-how-to-run)
7. [Output Files](#7-output-files)
8. [Phase 1 â€“ Build](#8-phase-1--build-synthetic-image-detector)
9. [Phase 2 â€“ Break](#9-phase-2--break-adversarial-evasion)
10. [Phase 3 â€“ Improve](#10-phase-3--improve-robustness-defense)
11. [Final Results](#11-final-results)
12. [Key Takeaways](#12-key-takeaways)
13. [References](#13-references)

---

## 1. Problem Overview

Modern generative AI models produce synthetic images that are nearly
indistinguishable from real photographs, creating serious risks in
cybersecurity, misinformation detection, and digital forensics.

This project follows a 3-phase research-inspired cycle:

| Phase | Name | Goal |
|-------|------|------|
| 1 | **Build** | Train a binary classifier: REAL vs AI-generated (FAKE) |
| 2 | **Break** | Craft adversarial modifications that fool the detector |
| 3 | **Improve** | Design and prototype a defense based on found weaknesses |

> This mirrors how real-world AI security tools evolve â€” build, get attacked,
> diagnose, harden.

---

## 2. Dataset

### CIFAKE â€“ Real and AI-Generated Synthetic Images

| Property       | Value                                         |
|----------------|-----------------------------------------------|
| Source         | Kaggle                                        |
| Total Images   | 120,000 (60,000 REAL + 60,000 FAKE)           |
| Resolution     | 32Ã—32 RGB (resized to 64Ã—64 during training)  |
| REAL source    | CIFAR-10 original photographs                 |
| FAKE source    | Stable Diffusion generated equivalents        |
| License        | CC0 Public Domain                             |
| Kaggle URL     | https://www.kaggle.com/datasets/birdy654/cifake-real-and-ai-generated-synthetic-images |

---

### How to Download

**Option A â€” Kaggle CLI (Recommended):**

```bash
# Step 1: Install Kaggle CLI
pip install kaggle

# Step 2: Place API key at:
#   Windows : C:\Users\<YourUsername>\.kaggle\kaggle.json
#   Linux   : ~/.kaggle/kaggle.json
# Get your key from: https://www.kaggle.com/settings â†’ API â†’ Create New Token

# Step 3: Download dataset
kaggle datasets download -d birdy654/cifake-real-and-ai-generated-synthetic-images

# Step 4: Extract (Windows PowerShell)
Expand-Archive -Path "cifake-real-and-ai-generated-synthetic-images.zip" `
               -DestinationPath "data"

# Step 4: Extract (Linux / macOS)
unzip cifake-real-and-ai-generated-synthetic-images.zip -d data
```

**Option B â€” Manual Download:**
1. Visit: https://www.kaggle.com/datasets/birdy654/cifake-real-and-ai-generated-synthetic-images
2. Click **Download** (requires Kaggle account)
3. Extract the zip into the `data/` folder

---

### Expected Folder Structure After Extraction

```
data/
â”œâ”€â”€ train/
â”‚   â”œâ”€â”€ REAL/     â† 50,000 images (.jpg)
â”‚   â””â”€â”€ FAKE/     â† 50,000 images (.jpg)
â””â”€â”€ test/
    â”œâ”€â”€ REAL/     â† 10,000 images (.jpg)
    â””â”€â”€ FAKE/     â† 10,000 images (.jpg)
```

> âš ï¸ `data/` is listed in `.gitignore` and is **NOT included** in this repo.
> You must download the dataset separately before running any phase.

---

## 3. Project Structure

```
cifake-detector/
â”‚
â”œâ”€â”€ data/                            â† NOT in repo (download from Kaggle)
â”‚   â”œâ”€â”€ train/
â”‚   â”‚   â”œâ”€â”€ REAL/
â”‚   â”‚   â””â”€â”€ FAKE/
â”‚   â””â”€â”€ test/
â”‚       â”œâ”€â”€ REAL/
â”‚       â””â”€â”€ FAKE/
â”‚
â”œâ”€â”€ models/                          â† NOT in repo (auto-created after training)
â”‚   â”œâ”€â”€ best.pth                     # Phase 1: trained ResNet-18 detector
â”‚   â”œâ”€â”€ robust.pth                   # Phase 3: adversarially hardened model
â”‚   â”œâ”€â”€ pgd_train_imgs.pt            # Cached PGD adversarial training images
â”‚   â”œâ”€â”€ pgd_train_lbls.pt
â”‚   â”œâ”€â”€ blur_train_imgs.pt           # Cached Blur augmented training images
â”‚   â”œâ”€â”€ blur_train_lbls.pt
â”‚   â”œâ”€â”€ adv_test_imgs.pt             # Cached PGD adversarial test images
â”‚   â”œâ”€â”€ adv_test_lbls.pt
â”‚   â”œâ”€â”€ blur_test_imgs.pt            # Cached Blur adversarial test images
â”‚   â””â”€â”€ blur_test_lbls.pt
â”‚
â”œâ”€â”€ outputs/                         â† NOT in repo (auto-created after running)
â”‚   â”œâ”€â”€ training_samples.png
â”‚   â”œâ”€â”€ test_samples.png
â”‚   â”œâ”€â”€ confusion_matrix.png
â”‚   â”œâ”€â”€ gradcam_fake.png
â”‚   â”œâ”€â”€ gradcam_real.png
â”‚   â”œâ”€â”€ saliency_fake.png
â”‚   â”œâ”€â”€ saliency_real.png
â”‚   â”œâ”€â”€ phase2/
â”‚   â”‚   â”œâ”€â”€ targets.png
â”‚   â”‚   â”œâ”€â”€ before_after.png
â”‚   â”‚   â”œâ”€â”€ confidence_trajectories.png
â”‚   â”‚   â”œâ”€â”€ gradcam_comparison.png
â”‚   â”‚   â”œâ”€â”€ fft_analysis.png
â”‚   â”‚   â””â”€â”€ attack_summary.png
â”‚   â””â”€â”€ phase3/
â”‚       â”œâ”€â”€ finetune_curves.png
â”‚       â”œâ”€â”€ robustness_comparison.png
â”‚       â”œâ”€â”€ confusion_matrices.png
â”‚       â””â”€â”€ adversarial_predictions.png
â”‚
â”œâ”€â”€ phase1.py                        # Train the detector (Build)
â”œâ”€â”€ phase1_eval.py                   # Evaluate model + generate explainability
â”œâ”€â”€ phase2.py                        # Adversarial evasion attacks (Break)
â”œâ”€â”€ phase3.py                        # Robustness defense + re-evaluation (Improve)
â”œâ”€â”€ requirements.txt                 # Python dependencies
â”œâ”€â”€ .gitignore                       # Excludes data/, models/, outputs/, venv/
â””â”€â”€ README.md                        # This file
```

---

## 4. Requirements

**`requirements.txt`:**

```
torch
torchvision
scikit-learn
matplotlib
tqdm
Pillow
opencv-python
numpy
```

### Tested Environment

| Component   | Version           |
|-------------|-------------------|
| Python      | 3.11              |
| PyTorch     | 2.x (CPU)         |
| Torchvision | 0.x compatible    |
| OS          | Windows 11        |

### GPU Installation (Optional â€” 10Ã— faster training)

```bash
# CUDA 11.8
pip install torch torchvision --index-url https://download.pytorch.org/whl/cu118

# CUDA 12.1
pip install torch torchvision --index-url https://download.pytorch.org/whl/cu121
```

---

## 5. Setup & Installation

```bash
# 1. Clone the repository
git clone https://github.com/<your-username>/cifake-detector.git
cd cifake-detector

# 2. Create a virtual environment
python -m venv venv

# Activate â€” Windows:
venv\Scripts\activate

# Activate â€” Linux / macOS:
source venv/bin/activate

# 3. Install all dependencies
pip install -r requirements.txt

# 4. Download and extract the CIFAKE dataset into data/
#    (see Section 2 above)

# 5. Verify your data path
python -c "import os; print(os.path.exists('data/train/REAL'))"
# Should print: True
```

---

## 6. How to Run

> Run each phase **in order**. Each phase depends on outputs from the previous one.

---

### Phase 1 â€” Train the Detector

```bash
python phase1.py
```

**What it does:**
- Loads CIFAKE train/val/test splits
- Downloads ResNet-18 pretrained weights (~45MB, one-time)
- Stage 1: trains classifier head only (5 epochs, frozen backbone)
- Stage 2: fine-tunes full network (5 epochs, low LR)
- Saves best checkpoint to `models/best.pth`
- Saves training curve plots to `outputs/`

> â± **Estimated time (CPU):** 2â€“3 hours for full 85k training set  
> ğŸ’¡ **Speed tip:** Add these two lines after the split in `phase1.py` to use 20% of data:
> ```python
> train_idx = train_idx[:int(len(train_idx) * 0.2)]
> val_idx   = val_idx[:int(len(val_idx)   * 0.2)]
> ```

---

### Phase 1 Eval â€” Evaluate + Explainability

```bash
python phase1_eval.py
```

**What it does:**
- Loads saved `models/best.pth` (no training)
- Runs inference on full 20k test set
- Prints Accuracy, Precision, Recall, F1, Classification Report
- Generates Grad-CAM heatmaps (FAKE + REAL)
- Generates Saliency Maps (FAKE + REAL)
- Saves confusion matrix and all visualizations to `outputs/`

> â± **Estimated time (CPU):** ~5 minutes

---

### Phase 2 â€” Adversarial Attacks

```bash
python phase2.py
```

**What it does:**
- Selects 10 high-confidence FAKE targets from test set (P(fake) â‰¥ 0.90)
- Runs 5 attack types on each target:
  - Gaussian Blur (k = 1â†’15)
  - JPEG Compression (quality = 20)
  - FGSM (Îµ = 0.005â†’0.10)
  - PGD (Îµ = 0.03 and 0.05, 40â€“50 iterations)
  - Blur + PGD (combined)
- Generates confidence trajectory plots per attack
- Generates Grad-CAM before/after comparison
- Generates FFT frequency spectrum analysis
- Produces full attack success summary
- Saves all outputs to `outputs/phase2/`

> â± **Estimated time (CPU):** ~20â€“30 minutes

---

### Phase 3 â€” Robustness Defense

```bash
python phase3.py
```

**What it does:**
- Generates 1000 PGD adversarial training examples (FAKE â†’ still FAKE)
- Generates 1000 Blur-augmented training examples (FAKE â†’ still FAKE)
- **Caches** all generated data to `models/*.pt` â€” re-runs load from cache instantly
- Builds combined training set (5k original + 1k PGD + 1k Blur = 7k total)
- Fine-tunes original model for 3 epochs (LR = 5e-5)
- Evaluates both original and robust model on:
  - Clean test set (20,000 images)
  - PGD adversarial test set (200 images)
  - Blur adversarial test set (200 images)
- Saves all comparison plots to `outputs/phase3/`

> â± **Estimated time (CPU):**
> - First run: ~40â€“50 minutes (includes PGD generation)
> - Re-run: ~15 minutes (loads from cache)

---

## 7. Output Files

### Phase 1 (`outputs/`)

| File | Description |
|------|-------------|
| `training_samples.png` | Grid of sample images from training set |
| `test_samples.png` | Grid of sample images from test set |
| `confusion_matrix.png` | 2Ã—2 confusion matrix on test set |
| `gradcam_fake.png` | Grad-CAM heatmaps for 6 FAKE test images |
| `gradcam_real.png` | Grad-CAM heatmaps for 6 REAL test images |
| `saliency_fake.png` | Saliency maps for 6 FAKE test images |
| `saliency_real.png` | Saliency maps for 6 REAL test images |
| `stage_1_head_only.png` | Loss + accuracy curves â€” Stage 1 training |
| `stage_2_fine-tuning.png` | Loss + accuracy curves â€” Stage 2 fine-tuning |

### Phase 2 (`outputs/phase2/`)

| File | Description |
|------|-------------|
| `targets.png` | 10 selected high-confidence FAKE targets |
| `before_after.png` | Original â†’ 4 attacks with confidence per target |
| `confidence_trajectories.png` | P(fake) vs attack strength (Blur/FGSM/PGD) |
| `gradcam_comparison.png` | Attention shift: original vs evaded image |
| `fft_analysis.png` | FFT frequency spectrum before and after attack |
| `attack_summary.png` | Evasion rate + avg confidence drop per attack |

### Phase 3 (`outputs/phase3/`)

| File | Description |
|------|-------------|
| `finetune_curves.png` | Loss/accuracy during adversarial fine-tuning |
| `robustness_comparison.png` | Bar chart: original vs robust across all conditions |
| `confusion_matrices.png` | Side-by-side confusion matrices (clean test set) |
| `adversarial_predictions.png` | PGD examples: original vs robust predictions |

---

## 8. Phase 1 â€“ Build: Synthetic Image Detector

### Model Architecture

```
Input: 64Ã—64 RGB image (normalized mean=0.5, std=0.5)
   â†“
ResNet-18 Backbone (ImageNet pretrained)
   â†“
Global Average Pooling â†’ 512-dim feature vector
   â†“
Dropout(0.3) â†’ Linear(512â†’256) â†’ ReLU â†’ Dropout(0.2) â†’ Linear(256â†’2)
   â†“
Output: [P(REAL), P(FAKE)]
```

### Training Strategy

| Stage | Backbone | Head | Epochs | LR |
|-------|----------|------|--------|----|
| 1 | Frozen | Trainable | 5 | 1e-3 |
| 2 | Trainable | Trainable | 5 | backbone: 1e-4, head: 1e-3 |

- **Early stopping:** patience = 3, based on validation F1
- **Scheduler:** StepLR, step=3, gamma=0.5

### Test Set Results

| Metric | Value |
|--------|-------|
| Accuracy | **97.11%** |
| Precision (FAKE) | 0.9758 |
| Recall (FAKE) | 0.9663 |
| F1-score (FAKE) | **0.9710** |

### Explainability Findings

From Grad-CAM and saliency maps:
- The model focuses heavily on **high-frequency edge and texture patterns**
  in FAKE images rather than semantic content (shapes, objects).
- REAL image attention is more diffuse and object-centered.
- This confirms the model learned **HF artifact fingerprints of generative models**
  â€” a significant latent vulnerability.

---

## 9. Phase 2 â€“ Break: Adversarial Evasion

### Target Selection

10 FAKE test images with **P(fake) = 1.0000** selected as attack targets.

### Attack Results

| Attack | Evaded / 10 | Evasion Rate | Avg Î”P(fake) |
|--------|:-----------:|:------------:|:------------:|
| Blur (k=15) | 9 / 10 | 90.0% | 0.7162 |
| JPEG (q=20) | 0 / 10 | 0.0% | 0.0000 |
| FGSM (Îµ=0.05) | 0 / 10 | 0.0% | 0.0202 |
| FGSM (Îµ=0.10) | 0 / 10 | 0.0% | ~0.000 |
| **PGD (Îµ=0.03)** | **10 / 10** | **100.0%** | **1.0000** |
| **PGD (Îµ=0.05)** | **10 / 10** | **100.0%** | **1.0000** |
| **Blur + PGD** | **10 / 10** | **100.0%** | **1.0000** |

### Why the Attacks Worked

| Attack | Mechanism | Evidence |
|--------|-----------|----------|
| **Blur** | Removes HF noise patterns (GAN/diffusion fingerprints) | FFT outer rings fade after blurring |
| **PGD** | Gradient-based: directly optimizes away the FAKE signal | Grad-CAM attention shifts to irrelevant regions |
| **Blur+PGD** | Blur removes HF artifacts; PGD erases remaining features | Most realistic adversarial images |
| **JPEG/FGSM** | Block artifacts don't match detector's cues; single-step too coarse | Consistently fail across all targets |

**Core finding:**
> The detector is HF-dependent and non-robust. It detects synthetic images based on
> superficial noise patterns, not semantic image understanding.

---

## 10. Phase 3 â€“ Improve: Robustness Defense

### Vulnerability Diagnosed

| Attack | Before Defense | Root Cause |
|--------|:--------------:|------------|
| PGD | 0% accuracy | Model relies on gradient-exploitable HF features |
| Blur | 27% accuracy | Model relies on HF artifact presence |

### Defense: Adversarial Fine-tuning

**Strategy directly derived from Phase 2 findings:**

```
Step 1: Generate 1000 PGD-perturbed FAKE training images
        â†’ Label them STILL FAKE
        â†’ Model learns to detect FAKEs even after gradient attack

Step 2: Generate 1000 Blur-augmented FAKE training images  
        â†’ Label them STILL FAKE
        â†’ Model learns to detect FAKEs without HF artifacts

Step 3: Mix with 5000 original clean training samples
        â†’ Total: 7000 examples (clean data stays dominant)

Step 4: Fine-tune original model â€” 3 epochs, LR=5e-5
        â†’ Low LR preserves clean accuracy
        â†’ New adversarial samples build robustness
```

### Results After Defense

| Condition | Original Model | Robust Model | Improvement |
|-----------|:--------------:|:------------:|:-----------:|
| Clean Test Set | 97.11% | **97.16%** | +0.04% |
| PGD Adversarial | 0.00% | **100.00%** | **+100.00%** |
| Blur Adversarial | 27.00% | **100.00%** | **+73.00%** |

### Why This Works

Adversarial training forces the model to find **deeper, more stable features**.
When it sees PGD-perturbed or blurred FAKE images that look nearly real, it can no
longer rely on HF noise â€” it must learn lower-frequency semantic cues
(color distributions, spatial coherence, object statistics).
Clean accuracy is preserved because clean data remains the majority of training.

---

## 11. Final Results

```
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘  PHASE     TASK                       KEY METRIC         â•‘
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
â•‘  Phase 1   Build Detector             Accuracy : 97.11%  â•‘
â•‘                                       F1 Score : 0.9710  â•‘
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
â•‘  Phase 2   PGD Attack Evasion Rate    100%               â•‘
â•‘            Blur Attack Evasion Rate    90%               â•‘
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
â•‘  Phase 3   Robust Model (Clean)       Accuracy : 97.16%  â•‘
â•‘            Robust Model (PGD)         Accuracy : 100%    â•‘
â•‘            Robust Model (Blur)        Accuracy : 100%    â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
```

---

## 12. Key Takeaways

- **ResNet-18 + transfer learning** achieves 97% accuracy on CIFAKE
  with only ~3 hours of CPU training.

- The detector is **critically vulnerable to PGD (100% evasion) and
  Gaussian blur (90% evasion)** â€” confirming it learned HF generative
  artifacts rather than true semantic content differences.

- **Only 3 fine-tuning epochs on 7000 samples** are sufficient to completely
  eliminate both vulnerabilities while preserving baseline accuracy.

- **Adversarial training is directly interpretable here** â€” because we know
  exactly which features the model relied on (HF artifacts), we can design
  training examples that specifically remove that reliance.

- This workflow â€” build â†’ attack â†’ diagnose â†’ harden â€” is identical to how
  production deepfake detectors and forensic AI tools are developed and maintained.

---

## 13. References

1. **CIFAKE Dataset:**  
   Bird, J. J. & Lotfi, A. (2023). *CIFAKE: Image Classification and Explainable
   Identification of AI-Generated Synthetic Images.* IEEE Access.  
   https://www.kaggle.com/datasets/birdy654/cifake-real-and-ai-generated-synthetic-images

2. **ResNet:**  
   He, K. et al. (2016). *Deep Residual Learning for Image Recognition.* CVPR.

3. **Grad-CAM:**  
   Selvaraju, R. R. et al. (2017). *Grad-CAM: Visual Explanations from Deep
   Networks via Gradient-Based Localization.* ICCV.

4. **PGD Attack:**  
   Madry, A. et al. (2018). *Towards Deep Learning Models Resistant to
   Adversarial Attacks.* ICLR.

5. **Adversarial Training:**  
   Goodfellow, I. et al. (2015). *Explaining and Harnessing Adversarial
   Examples.* ICLR.

6. **FGSM:**  
   Goodfellow, I. et al. (2015). *Explaining and Harnessing Adversarial
   Examples.* ICLR.

---

> **Submission includes:** `phase1.py` Â· `phase1_eval.py` Â· `phase2.py` Â· `phase3.py` Â· `requirements.txt` Â· `README.md`
```

***